# BAYESIAN ENTROPY FOR LLM HALLUCINATION DETECTION

# ğŸ§  LLM-HALLUCINATION

This repository explores **hallucination detection and mitigation** in Large Language Models (LLMs) using Bayesian estimators, adaptive sampling, and evaluation over multiple QA datasets (SQuAD, SVAMP, TriviaQA).

---

## ğŸ“ Project Structure
```
LLM-HALLUCINATION/
â”œâ”€â”€ .vscode/
â”‚   â””â”€â”€ settings.json
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â””â”€â”€ triviaqa.yaml
â”‚   â”œâ”€â”€ estimator/
â”‚   â”‚   â”œâ”€â”€ bayes_default.yaml
â”‚   â”‚   â”œâ”€â”€ histogram.yaml
â”‚   â”‚   â””â”€â”€ rescaled.yaml
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ adaptive_budget.yaml
â”‚   â”‚   â””â”€â”€ fixed_budget.yaml
â”‚   â””â”€â”€ model/
â”‚       â”œâ”€â”€ base_config.yaml
â”‚       â”œâ”€â”€ llama2.yaml
â”‚       â””â”€â”€ mistral.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generations/
â”‚   â”œâ”€â”€ meanings/
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ squad_prompts.json
â”‚   â”‚   â”œâ”€â”€ svamp_prompts.json
â”‚   â”‚   â””â”€â”€ triviaqa_prompts.json
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ SQuAD/
â”‚       â”‚   â”œâ”€â”€ dev-v2.0.json
â”‚       â”‚   â””â”€â”€ SQuAD.ipynb
â”‚       â”œâ”€â”€ SVAMP/
â”‚       â”‚   â”œâ”€â”€ SVAMP.json
â”‚       â”‚   â””â”€â”€ SVAMP.ipynb
â”‚       â””â”€â”€ TriviaQA/
â”‚           â”œâ”€â”€ TriviaQA.json
â”‚           â””â”€â”€ TriviaQA.ipynb
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ analyze_results.py
â”‚   â”œâ”€â”€ run_adaptive_budget.py
â”‚   â””â”€â”€ run_fixed_budget.py
â”œâ”€â”€ pdfs/
â”œâ”€â”€ src_code/
â”‚   â”œâ”€â”€ bayesian_estimator/
â”‚   â”‚   â”œâ”€â”€ dirichlet.py
â”‚   â”‚   â”œâ”€â”€ estimator.py
â”‚   â”‚   â”œâ”€â”€ hierarchical_model.py
â”‚   â”‚   â”œâ”€â”€ truncated_dirichlet.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ compare_baseline.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ visualize_results.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ phi-2.Q4_K_M.gguf
â”‚   â”‚   â”œâ”€â”€ qwen1_5-0.5b-chat-q4_k_m.gguf
â”‚   â”‚   â”œâ”€â”€ tinyllama-1.1b-chat-v1.0.Q4_0.gguf
â”‚   â”‚   â”œâ”€â”€ phi.py
â”‚   â”‚   â”œâ”€â”€ qwen.py
â”‚   â”‚   â””â”€â”€ tinyllama.py
â”‚   â”œâ”€â”€ adaptive_sampler.py
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”œâ”€â”€ estimate_entropy.py
â”‚   â”œâ”€â”€ meaning_mapper.py
â”‚   â””â”€â”€ train_prior.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md

```




---

## âš™ï¸ Setup

### 1. Create Conda Environment
```bash
conda create -n llmhall python=3.11
conda activate llmhall
pip install -r requirements.txt
```

If youâ€™re using llama-cpp-python or similar:  Install Visual Studio Build Tools with Desktop Development with C++.
```pip install llama-cpp-python --force-reinstall --no-cache-dir```






